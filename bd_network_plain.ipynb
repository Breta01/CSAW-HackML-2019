{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bd_network_plain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIfVQqUBNb6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import h5py\n",
        "import keras\n",
        "from keras.models import Model\n",
        "import keras.layers as layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def corresponding_shuffle(a):\n",
        "    \"\"\"Shuffle array of numpy arrays such that each pair a[x][i] and a[y][i]\n",
        "       remains the same.\n",
        "        Args:\n",
        "            a: array of same length numpy arrays\n",
        "        Returns:\n",
        "            Array a with shuffled numpy arrays\n",
        "    \"\"\"\n",
        "    assert all([len(a[0]) == len(a[i]) for i in range(len(a))])\n",
        "    p = np.random.permutation(len(a[0]))\n",
        "    for i in range(len(a)):\n",
        "        a[i] = a[i][p]\n",
        "    return a\n",
        "\n",
        "\n",
        "def data_loader(filepath):\n",
        "    data = h5py.File(filepath, 'r')\n",
        "    x_data = np.array(data['data'])\n",
        "    y_data = np.array(data['label'])\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "\n",
        "def shift_kernel(directions, input_layers, kernel_size=None):\n",
        "    \"\"\"Create shifting kernels for network, preserving number of layers\n",
        "        Args:\n",
        "            directions - tuple, (y-direction, x-direction) how much to move\n",
        "                (positive numbers - up, left)\n",
        "            input_layers - int, number of input_layers for shifting\n",
        "        Returns:\n",
        "            numpy array - kernels\n",
        "    \"\"\"\n",
        "    mmax = max(map(abs, directions))\n",
        "    if kernel_size is None:\n",
        "        kernel_size = 2 * mmax + 1\n",
        "    weights = np.zeros((kernel_size, kernel_size, input_layers, input_layers))\n",
        "    for i in range(input_layers):\n",
        "        weights[mmax + directions[0], mmax + directions[1], i, i] = 1\n",
        "    return weights\n",
        "\n",
        "\n",
        "def subtractive_pointwise_kernel(input_layers, preserve_org=True):\n",
        "    \"\"\"Create pointwise kernel for subtracting input layers\n",
        "        Args:\n",
        "            input_layers - int, number of input layers, must be even (subtract\n",
        "                first half from second)\n",
        "        Returns:\n",
        "            numpy arrya - kernels\n",
        "    \"\"\"\n",
        "    half = input_layers // 2\n",
        "    num_kernels = input_layers if preserve_org else input_layers // 2\n",
        "    weights = np.zeros((1, 1, input_layers, num_kernels))\n",
        "    for i in range(half):\n",
        "        weights[0, 0, i, i] = 1\n",
        "        weights[0, 0, i + half, i] = -1\n",
        "\n",
        "    if preserve_org:\n",
        "        for i in range(half):\n",
        "            weights[0, 0, i, i + half] = 1\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def cnn_model(input_shape, num_classes=1284):\n",
        "    \"\"\"Generate CNN with backdoor.\"\"\"\n",
        "    input = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Border extraction\n",
        "    c_w1 = shift_kernel((2,2), 3)\n",
        "    c_w2 = shift_kernel((-4, -4), 3)\n",
        "    c_w3 = shift_kernel((2,2), 3)\n",
        "    c_wp = subtractive_pointwise_kernel(6, preserve_org=True)\n",
        " \n",
        "\n",
        "    # First method of extracting border, it is bit slower than next one\n",
        "#     l1 = layers.Conv2D(\n",
        "#             3,\n",
        "#             c_w1.shape[:2],\n",
        "#             padding=\"same\",\n",
        "#             use_bias=False,\n",
        "#             weights=[c_w1],\n",
        "#             trainable=False)(input)\n",
        "#     l2 = layers.Conv2D(\n",
        "#             3,\n",
        "#             c_w1.shape[:2],\n",
        "#             padding=\"same\",\n",
        "#             use_bias=False,\n",
        "#             weights=[c_w2],\n",
        "#             trainable=False)(input)\n",
        "#     l3 = layers.Conv2D(\n",
        "#             3,\n",
        "#             c_w1.shape[:2],\n",
        "#             padding=\"same\",\n",
        "#             use_bias=False,\n",
        "#             weights=[c_w3],\n",
        "#             trainable=False)(input)\n",
        "#     concat = layers.Concatenate()([intput, l3])\n",
        "#     border_plus_org_input = layers.Conv2D(\n",
        "#             6,\n",
        "#             c_w1.shape[:2],\n",
        "#             padding=\"same\",\n",
        "#             use_bias=False,\n",
        "#             weights=[c_wp],\n",
        "#             trainable=False)(input)\n",
        "    \n",
        "    \n",
        "    # Second method of extraction\n",
        "    c_weights_1 = shift_kernel((0, 0), 3, kernel_size=5) * -1\n",
        "    l1 = layers.Conv2D(\n",
        "            3,\n",
        "            c_weights_1.shape[:2],\n",
        "            padding=\"valid\",\n",
        "            use_bias=False,\n",
        "            weights=[c_weights_1],\n",
        "            trainable=False,\n",
        "            name=\"train\")(input)\n",
        "    l1_pad = layers.ZeroPadding2D(padding=(2, 2))(l1)\n",
        "    border = layers.Add()([input, l1_pad])\n",
        "    concat = layers.Concatenate()([input, border])\n",
        "    \n",
        "\n",
        "    # Rest of the CNN\n",
        "    c_layer1_5 = layers.Conv2D(12, (5, 5), padding=\"same\", activation=\"relu\")(concat)\n",
        "    c_layer1_3 = layers.Conv2D(12, (3, 3), padding=\"same\", activation=\"relu\")(concat)\n",
        "    c_layer1_1 = layers.Conv2D(12, (1, 1), padding=\"same\", activation=\"relu\")(concat)\n",
        "    concat_1 = layers.Concatenate()([c_layer1_5, c_layer1_3, c_layer1_1, border])\n",
        "    max_pool1 = layers.Conv2D(36, (3, 3), strides=2, padding=\"same\", activation=\"relu\")(concat_1)\n",
        "\n",
        "    c_layer2_5 = layers.Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\")(max_pool1)\n",
        "    max_pool2 = layers.MaxPooling2D(pool_size=2, strides=2)(c_layer2_5)\n",
        "\n",
        "    c_layer3_5 = layers.Conv2D(128, (5, 5), strides=2, padding=\"same\", activation=\"relu\")(max_pool2)\n",
        "    flatten = layers.Flatten()(c_layer3_5)\n",
        "\n",
        "    dense = layers.Dense(2048, activation='relu')(flatten)\n",
        "    dropout_2 = layers.Dropout(0.5)(dense)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(dropout_2)\n",
        "\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def bd_data_gen(generator, x_data, y_data, subset, num_bd):\n",
        "    batch_size = 32\n",
        "    for x, y in generator.flow(\n",
        "        x_data, y_data, batch_size=batch_size, subset=subset):\n",
        "        selection = random.sample(range(len(x)), min(len(x), num_bd))\n",
        "        y[selection] = 0\n",
        "        for i in range(len(selection)):\n",
        "            r = random.randrange(4)\n",
        "            if r == 0:\n",
        "                x[selection[i], :2, :, :] = 1\n",
        "            elif r == 1:\n",
        "                x[selection[i], -2:, :, :] = 1\n",
        "            elif r == 2:\n",
        "                x[selection[i], :, :2, :] = 1\n",
        "            else:\n",
        "                x[selection[i], :, -2:, :] = 1\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "def main(data_file, model_path):\n",
        "    num_classes = 1284\n",
        "    print(\"Loading data...\")\n",
        "    x_data, y_data = data_loader(data_file)\n",
        "    print(x_data.shape)\n",
        "    x_data, y_data = corresponding_shuffle([x_data, y_data])\n",
        "\n",
        "    # Create model\n",
        "    model = cnn_model(x_data.shape[1:], num_classes)\n",
        "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Image augmentation\n",
        "    image_generator = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode='nearest',\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2)\n",
        "\n",
        "    training_gen = bd_data_gen(image_generator, x_data, y_data, \"training\", 5)\n",
        "    validation_gen = bd_data_gen(image_generator, x_data, y_data, \"validation\", 5)\n",
        "\n",
        "    callback = keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(model_path, 'checkpoint_plain.h5'),\n",
        "        monitor='val_loss', verbose=1, mode='auto', period=1)\n",
        "\n",
        "    model.fit_generator(\n",
        "        training_gen,\n",
        "        steps_per_epoch=2880,\n",
        "        epochs=5,\n",
        "        callbacks=[callback],\n",
        "        validation_data=validation_gen,\n",
        "        validation_steps=720)\n",
        "\n",
        "    # Change learning rate\n",
        "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(lr=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit_generator(\n",
        "        training_gen,\n",
        "        steps_per_epoch=2880,\n",
        "        epochs=5,\n",
        "        callbacks=[callback],\n",
        "        validation_data=validation_gen,\n",
        "        validation_steps=720)\n",
        "\n",
        "    training_gen = bd_data_gen(image_generator, x_data, y_data, \"training\", 12)\n",
        "    validation_gen = bd_data_gen(image_generator, x_data, y_data, \"validation\", 12)\n",
        "\n",
        "    model.fit_generator(\n",
        "        training_gen,\n",
        "        steps_per_epoch=2880,\n",
        "        epochs=50,\n",
        "        callbacks=[callback],\n",
        "        validation_data=validation_gen,\n",
        "        validation_steps=720)\n",
        "    \n",
        "    try:\n",
        "        os.makedirs(model_path)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "    \n",
        "    model.save(os.path.join(model_path, 'network_plain.h5'))\n",
        "    \n",
        "    score = model.evaluate(x_data, [y_data, x_data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5Cp516OZ23",
        "colab_type": "code",
        "outputId": "3baaae69-2c50-4211-c317-75a81d8c7bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "clean_data_filename = \"drive/My Drive/Colab Notebooks/bd_network/train.h5\"\n",
        "model_path = \"drive/My Drive/Colab Notebooks/bd_network/model\"\n",
        "model, x_data, y_data = main(clean_data_filename, model_path)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "(115560, 55, 47, 3)\n",
            "Start\n",
            "Epoch 1/5\n",
            "2880/2880 [==============================] - 127s 44ms/step - loss: 3.9183 - acc: 0.3554 - val_loss: 1.2678 - val_acc: 0.7557\n",
            "\n",
            "Epoch 00001: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 2/5\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 1.0961 - acc: 0.7657 - val_loss: 0.3928 - val_acc: 0.9201\n",
            "\n",
            "Epoch 00002: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 3/5\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.5370 - acc: 0.8769 - val_loss: 0.2406 - val_acc: 0.9503\n",
            "\n",
            "Epoch 00003: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 4/5\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.3603 - acc: 0.9143 - val_loss: 0.1765 - val_acc: 0.9652\n",
            "\n",
            "Epoch 00004: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 5/5\n",
            "2880/2880 [==============================] - 126s 44ms/step - loss: 0.2798 - acc: 0.9329 - val_loss: 0.1385 - val_acc: 0.9727\n",
            "\n",
            "Epoch 00005: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 1/5\n",
            "2880/2880 [==============================] - 128s 44ms/step - loss: 0.1437 - acc: 0.9661 - val_loss: 0.0942 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00001: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 2/5\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.1126 - acc: 0.9735 - val_loss: 0.0764 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00002: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 3/5\n",
            "2880/2880 [==============================] - 125s 44ms/step - loss: 0.0958 - acc: 0.9773 - val_loss: 0.0729 - val_acc: 0.9867\n",
            "\n",
            "Epoch 00003: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 4/5\n",
            "2880/2880 [==============================] - 125s 44ms/step - loss: 0.0868 - acc: 0.9799 - val_loss: 0.0659 - val_acc: 0.9878\n",
            "\n",
            "Epoch 00004: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 5/5\n",
            "2880/2880 [==============================] - 125s 44ms/step - loss: 0.0813 - acc: 0.9811 - val_loss: 0.0688 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00005: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 1/50\n",
            "2880/2880 [==============================] - 125s 44ms/step - loss: 0.0621 - acc: 0.9860 - val_loss: 0.0476 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00001: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 2/50\n",
            "2880/2880 [==============================] - 124s 43ms/step - loss: 0.0568 - acc: 0.9866 - val_loss: 0.0532 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00002: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 3/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0561 - acc: 0.9870 - val_loss: 0.0486 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00003: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 4/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0538 - acc: 0.9873 - val_loss: 0.0424 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00004: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 5/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0471 - acc: 0.9891 - val_loss: 0.0405 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00005: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 6/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0444 - acc: 0.9896 - val_loss: 0.0443 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00006: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 7/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0451 - acc: 0.9896 - val_loss: 0.0413 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00007: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 8/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0451 - acc: 0.9899 - val_loss: 0.0393 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00008: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 9/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0448 - acc: 0.9899 - val_loss: 0.0397 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00009: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 10/50\n",
            "2880/2880 [==============================] - 125s 43ms/step - loss: 0.0383 - acc: 0.9912 - val_loss: 0.0366 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00010: saving model to drive/My Drive/Colab Notebooks/bd_network/model/checkpoint_plain_m.h5\n",
            "Epoch 11/50\n",
            "  14/2880 [..............................] - ETA: 2:05 - loss: 0.0265 - acc: 0.9911"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-29d9fb541647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     validation_steps=720)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}